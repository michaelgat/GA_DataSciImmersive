{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT - PART 4\n",
    "__Michael Gat__  \n",
    "__General Assembly Santa Monica, Data Science Immersive, Summer 2016__\n",
    "\n",
    "In this notebook, we'll build upon __Part_3__ by adding oversampling as suggested in [Predictive risk modelling for early hospital readmission of patients with diabetes in India](http://link.springer.com/article/10.1007/s13410-016-0511-8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES ############################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ IN DATA\n",
    "We have a clean dataset from __Part 1__. We'll import that, then select only the columns we want to deal with at this time. This code is similar to __Part_2__, please review that notebook for additional details/comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetic_data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_model_1 = df.ix[:,0:15]\n",
    "df_model_1 = df_model_1.join(df.ix[:,['change', 'diabetesMed', 'age_group', 'readmit']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD FEATURE SELECTION\n",
    "Still working with the recommended Chi2, test for different numbers of features, using code adapted from __Part2__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_model_1.ix[:,0:17]\n",
    "y = df_model_1.ix[:,18]\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling\n",
    "Use the random under sampler to train the model on data that is evenly balanced between the two possible target results. Given the healthy size of the training set, undersampling the negatives should yield a sufficient sample for training and is preferable in this case to attempting an oversample of the positive results.\n",
    "\n",
    "The idea behind undersampling is relatively simple, but implementations can be complex depending on the circumstances and the desired result. It is difficult to train a categorical model when the datasets is significantly out of balance, as this dataset is. As established in __Part 1A__, approximately 90% of the records in the set are \"no readmit\" (0 in the target variable). Most models will not handle very sparse positive results very well, as they will tend to discount the possibility of positives occuring. Much better results can be achieved when the training data is artificially balanced even when the test data is not! This avoids biasing the model towards a particular mix of results and forces it to focus exclusively on the predictive variables.\n",
    "\n",
    "Oversampling can be used in smaller datasets when it is not possible to reduce the prevalance of one target variable while maintaining a statistically-significant set of training data. In oversampling, artificial additional datapoints are generated to \"fill out\" the under-represented target variable.\n",
    "\n",
    "As the dataset for this project is fairly large, this is not a significant concern, we can select a random subset (approximately 10%) from the \"no readmit\" category to maintain numerical balance with the \"readmit\" category. The imbalanced-learn package does this for us using the \"random under sampler.\" Other over and under sampling methods are available as well, but there was inadequate time to explore other options.\n",
    "\n",
    "Examples and documentation of the imbalanced-learn methods are available at [http://contrib.scikit-learn.org/imbalanced-learn/auto_examples/index.html#example-using-under-sampling-class-methods](http://contrib.scikit-learn.org/imbalanced-learn/auto_examples/index.html#example-using-under-sampling-class-methods)\n",
    "\n",
    "Key results from a test run are in __Capstone_Results.xlsx__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use random under sampler module to reduce the number of \"no readmit\" records influencing the learning\n",
    "rus = RandomUnderSampler()\n",
    "X_resampled, y_resampled = rus.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECTED FEATURES:3\n",
      "discharge_disposition_id\n",
      "number_emergency\n",
      "number_inpatient\n",
      "[  6.63668475e-01   9.80655047e-02   2.86383524e+00   1.05855045e+03\n",
      "   1.21419430e+01   1.99441011e+02   2.76587759e+02   2.50889156e+01\n",
      "   3.49720813e+02   5.92906597e+01   7.69714668e+02   2.58585905e+03\n",
      "   6.92053231e+01   3.08454810e+00   6.61144219e+00   1.52367438e+01\n",
      "   9.62466307e+00]\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "0.660661644284\n",
      "[[45673 22175]\n",
      " [ 3725  4752]]\n",
      "AUC Metrics:\n",
      "0.616871082579\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features=1, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.650337373076\n",
      "[[44752 23096]\n",
      " [ 3592  4885]]\n",
      "AUC Metrics:\n",
      "0.617928608699\n",
      "\n",
      "GaussianNB()\n",
      "0.791523092041\n",
      "[[57984  9864]\n",
      " [ 6048  2429]]\n",
      "AUC Metrics:\n",
      "0.570578125233\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.713265640354\n",
      "[[50750 17098]\n",
      " [ 4787  3690]]\n",
      "AUC Metrics:\n",
      "0.591645512441\n",
      "\n",
      "SELECTED FEATURES:4\n",
      "discharge_disposition_id\n",
      "num_medications\n",
      "number_emergency\n",
      "number_inpatient\n",
      "[  6.63668475e-01   9.80655047e-02   2.86383524e+00   1.05855045e+03\n",
      "   1.21419430e+01   1.99441011e+02   2.76587759e+02   2.50889156e+01\n",
      "   3.49720813e+02   5.92906597e+01   7.69714668e+02   2.58585905e+03\n",
      "   6.92053231e+01   3.08454810e+00   6.61144219e+00   1.52367438e+01\n",
      "   9.62466307e+00]\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "0.662980674746\n",
      "[[45844 22004]\n",
      " [ 3719  4758]]\n",
      "AUC Metrics:\n",
      "0.618485151155\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features=1, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.60188666885\n",
      "[[40609 27239]\n",
      " [ 3147  5330]]\n",
      "AUC Metrics:\n",
      "0.613644619779\n",
      "\n",
      "GaussianNB()\n",
      "0.795414346544\n",
      "[[58283  9565]\n",
      " [ 6050  2427]]\n",
      "AUC Metrics:\n",
      "0.572663613752\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.690926957091\n",
      "[[48772 19076]\n",
      " [ 4514  3963]]\n",
      "AUC Metrics:\n",
      "0.593171206295\n",
      "\n",
      "SELECTED FEATURES:5\n",
      "discharge_disposition_id\n",
      "num_lab_procedures\n",
      "num_medications\n",
      "number_emergency\n",
      "number_inpatient\n",
      "[  6.63668475e-01   9.80655047e-02   2.86383524e+00   1.05855045e+03\n",
      "   1.21419430e+01   1.99441011e+02   2.76587759e+02   2.50889156e+01\n",
      "   3.49720813e+02   5.92906597e+01   7.69714668e+02   2.58585905e+03\n",
      "   6.92053231e+01   3.08454810e+00   6.61144219e+00   1.52367438e+01\n",
      "   9.62466307e+00]\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "0.638873239437\n",
      "[[43736 24112]\n",
      " [ 3451  5026]]\n",
      "AUC Metrics:\n",
      "0.618757905537\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features=1, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.619194235179\n",
      "[[42065 25783]\n",
      " [ 3282  5195]]\n",
      "AUC Metrics:\n",
      "0.616411763879\n",
      "\n",
      "GaussianNB()\n",
      "0.794628234523\n",
      "[[58211  9637]\n",
      " [ 6038  2439]]\n",
      "AUC Metrics:\n",
      "0.572840813515\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.689407140518\n",
      "[[48632 19216]\n",
      " [ 4490  3987]]\n",
      "AUC Metrics:\n",
      "0.593555083477\n",
      "\n",
      "SELECTED FEATURES:6\n",
      "discharge_disposition_id\n",
      "time_in_hospital\n",
      "num_lab_procedures\n",
      "num_medications\n",
      "number_emergency\n",
      "number_inpatient\n",
      "[  6.63668475e-01   9.80655047e-02   2.86383524e+00   1.05855045e+03\n",
      "   1.21419430e+01   1.99441011e+02   2.76587759e+02   2.50889156e+01\n",
      "   3.49720813e+02   5.92906597e+01   7.69714668e+02   2.58585905e+03\n",
      "   6.92053231e+01   3.08454810e+00   6.61144219e+00   1.52367438e+01\n",
      "   9.62466307e+00]\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "0.61449066492\n",
      "[[41608 26240]\n",
      " [ 3184  5293]]\n",
      "AUC Metrics:\n",
      "0.618824288509\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features=1, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.583596462496\n",
      "[[38917 28931]\n",
      " [ 2851  5626]]\n",
      "AUC Metrics:\n",
      "0.618634578042\n",
      "\n",
      "GaussianNB()\n",
      "0.795414346544\n",
      "[[58256  9592]\n",
      " [ 6023  2454]]\n",
      "AUC Metrics:\n",
      "0.574057184107\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.688005240747\n",
      "[[48487 19361]\n",
      " [ 4452  4025]]\n",
      "AUC Metrics:\n",
      "0.594727877421\n",
      "\n",
      "SELECTED FEATURES:7\n",
      "discharge_disposition_id\n",
      "time_in_hospital\n",
      "num_lab_procedures\n",
      "num_medications\n",
      "number_emergency\n",
      "number_inpatient\n",
      "number_diagnoses\n",
      "[  6.63668475e-01   9.80655047e-02   2.86383524e+00   1.05855045e+03\n",
      "   1.21419430e+01   1.99441011e+02   2.76587759e+02   2.50889156e+01\n",
      "   3.49720813e+02   5.92906597e+01   7.69714668e+02   2.58585905e+03\n",
      "   6.92053231e+01   3.08454810e+00   6.61144219e+00   1.52367438e+01\n",
      "   9.62466307e+00]\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "0.607284638061\n",
      "[[40975 26873]\n",
      " [ 3101  5376]]\n",
      "AUC Metrics:\n",
      "0.619055049316\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features=1, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.589911562398\n",
      "[[39635 28213]\n",
      " [ 3087  5390]]\n",
      "AUC Metrics:\n",
      "0.610005798408\n",
      "\n",
      "GaussianNB()\n",
      "0.795047494268\n",
      "[[58234  9614]\n",
      " [ 6029  2448]]\n",
      "AUC Metrics:\n",
      "0.573541158214\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.680799213888\n",
      "[[47847 20001]\n",
      " [ 4362  4115]]\n",
      "AUC Metrics:\n",
      "0.595319934245\n",
      "\n",
      "SELECTED FEATURES:8\n",
      "discharge_disposition_id\n",
      "time_in_hospital\n",
      "num_lab_procedures\n",
      "num_medications\n",
      "number_outpatient\n",
      "number_emergency\n",
      "number_inpatient\n",
      "number_diagnoses\n",
      "[  6.63668475e-01   9.80655047e-02   2.86383524e+00   1.05855045e+03\n",
      "   1.21419430e+01   1.99441011e+02   2.76587759e+02   2.50889156e+01\n",
      "   3.49720813e+02   5.92906597e+01   7.69714668e+02   2.58585905e+03\n",
      "   6.92053231e+01   3.08454810e+00   6.61144219e+00   1.52367438e+01\n",
      "   9.62466307e+00]\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "0.608031444481\n",
      "[[41033 26815]\n",
      " [ 3102  5375]]\n",
      "AUC Metrics:\n",
      "0.619423492196\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features=1, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.5887585981\n",
      "[[39483 28365]\n",
      " [ 3023  5454]]\n",
      "AUC Metrics:\n",
      "0.612660567855\n",
      "\n",
      "GaussianNB()\n",
      "0.794981984933\n",
      "[[58223  9625]\n",
      " [ 6023  2454]]\n",
      "AUC Metrics:\n",
      "0.573813993446\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.680524074681\n",
      "[[47821 20027]\n",
      " [ 4357  4120]]\n",
      "AUC Metrics:\n",
      "0.595423245136\n",
      "\n",
      "SELECTED FEATURES:9\n",
      "discharge_disposition_id\n",
      "time_in_hospital\n",
      "num_lab_procedures\n",
      "num_procedures\n",
      "num_medications\n",
      "number_outpatient\n",
      "number_emergency\n",
      "number_inpatient\n",
      "number_diagnoses\n",
      "[  6.63668475e-01   9.80655047e-02   2.86383524e+00   1.05855045e+03\n",
      "   1.21419430e+01   1.99441011e+02   2.76587759e+02   2.50889156e+01\n",
      "   3.49720813e+02   5.92906597e+01   7.69714668e+02   2.58585905e+03\n",
      "   6.92053231e+01   3.08454810e+00   6.61144219e+00   1.52367438e+01\n",
      "   9.62466307e+00]\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "0.608673435965\n",
      "[[41092 26756]\n",
      " [ 3112  5365]]\n",
      "AUC Metrics:\n",
      "0.619268456313\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=7, max_features=1, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.577884048477\n",
      "[[38569 29279]\n",
      " [ 2939  5538]]\n",
      "AUC Metrics:\n",
      "0.610879506463\n",
      "\n",
      "GaussianNB()\n",
      "0.795519161481\n",
      "[[58264  9584]\n",
      " [ 6023  2454]]\n",
      "AUC Metrics:\n",
      "0.574116139419\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.678886341304\n",
      "[[47666 20182]\n",
      " [ 4327  4150]]\n",
      "AUC Metrics:\n",
      "0.596050479893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [DecisionTreeClassifier(max_depth=7), \\\n",
    "RandomForestClassifier(max_depth=7, n_estimators=10, max_features=1), \\\n",
    "GaussianNB(), LogisticRegression()]\n",
    "\n",
    "for numtest in range (3,10):\n",
    "    ch2 = SelectKBest(chi2, k=numtest)\n",
    "    X_train_fit = ch2.fit_transform(X_resampled, y_resampled)\n",
    "    print 'SELECTED FEATURES:' + str(numtest)\n",
    "    col_indices = ch2.get_support(indices=True)\n",
    "    for i in col_indices:\n",
    "        print X_train.columns.values[i]\n",
    "    print ch2.scores_\n",
    "    print\n",
    "    \n",
    "    X_test_xform = ch2.transform(X_test)\n",
    "    \n",
    "    for clf in classifiers:\n",
    "        clf.fit(X_train_fit, y_resampled)\n",
    "        score = clf.score(X_test_xform, y_test)\n",
    "        y_pred = clf.predict(X_test_xform)\n",
    "        y_pred = clf.predict(X_test_xform)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        print clf\n",
    "        print score\n",
    "        print cm\n",
    "        print \"AUC Metrics:\"\n",
    "        print auc(fpr, tpr)\n",
    "        print"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
